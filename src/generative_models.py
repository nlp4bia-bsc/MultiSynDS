from transformers import pipeline

class LlamaInstruct:
    
    def __init__(self, model_id: str, device: str = "cuda"):
        self.model_id = model_id
        self.device = device
        self.model = self.load_model()

    def load_model(self) -> pipeline:
        """
        Load the LLAMA model from Hugging Face.
        
        Args:
            model_id (str): Hugging Face model ID.
        
        Returns:
            transformers.pipeline: LLAMA model pipeline.
        """
        from transformers import pipeline
        
        model = pipeline("text-generation", model=self.model_id,  pad_token_id=128001, device=self.device)
        
        print(f"Model {self.model_id} loaded successfully.")
        return model
    
    def generate(self, prompt, system="", **kwargs):
        """
        Generate text output from the LLAMA model.
        
        Args:
            pipe (transformers.pipeline): LLAMA model pipeline.
            prompt (str): Input text prompt.
        
        Returns:
            str: Output text generated by the model.
        """
        messages = [{"role": "system", "content": system},
                    {"role": "user", "content": prompt}]
        
        output = self.model(messages, **kwargs)

        return output[-1]["generated_text"][-1]["content"]