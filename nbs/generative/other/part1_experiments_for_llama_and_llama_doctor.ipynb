{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 17:02:21,650 - INFO - Configuration Settings:\n",
      "2025-04-03 17:02:21,651 - INFO - N_EXPECTED_SAMPLES: 35\n",
      "2025-04-03 17:02:21,652 - INFO - N_EXAMPLES: 7\n",
      "2025-04-03 17:02:21,652 - INFO - MODEL_ID: /gpfs/projects/bsc14/abecerr1/hub/models--Henrychur--MMed-Llama-3-8B-EnIns/snapshots/45936f724a7eabfce59cd8eaed318970db468cad\n",
      "2025-04-03 17:02:21,652 - INFO - SOURCE_PATH: output/samples/en/phase_2/\n",
      "2025-04-03 17:02:21,653 - INFO - TEMPLATES_PATH: utils/templates/basic\n",
      "2025-04-03 17:02:21,653 - INFO - OUTPUT_PATH: output/evaluation/MMed-Llama-3-8B-EnIns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\".\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from src.data import files_to_df, Prompt, create_examples\n",
    "from src.generative_models import LlamaInstruct\n",
    "from src.generate import safe_generate\n",
    "from src.utils import setup_logger, log_info, path_with_datetime, load_config, log_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(config_path=\"scripts/MMed-Llama-3-8B-EnIns/config.yaml\")\n",
    "\n",
    "N_EXPECTED_SAMPLES = config[\"N_EXPECTED_SAMPLES\"]\n",
    "# N_EXAMPLES = config[\"N_EXAMPLES\"]\n",
    "N_EXAMPLES = 0\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "SOURCE_PATH = config[\"SOURCE_PATH\"]\n",
    "TEMPLATES_PATH = config[\"TEMPLATES_PATH\"]\n",
    "OUTPUT_PATH = path_with_datetime(config[\"OUTPUT_PATH\"])\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# write config to file in OUTPUT_PATH\n",
    "with open(os.path.join(OUTPUT_PATH, \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "        \n",
    "# Setup logger\n",
    "setup_logger(os.path.join(OUTPUT_PATH, \"app.log\"))\n",
    "log_config(config)\n",
    "\n",
    "def load_file_content(filepath):\n",
    "    \"\"\"Safely load text file content.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return file.read().strip()  # Strip trailing spaces\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load generated, original, human evaluation, and automatic evaluation datasets.\n",
    "    \n",
    "    Returns:\n",
    "        df_pairs: pd.DataFrame, pairs of generated and original samples\n",
    "        df_human: pd.DataFrame, human evaluation scores\n",
    "        df_auto: pd.DataFrame, automatic evaluation scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load generated and original datasets and format it\n",
    "    df_gen = files_to_df(os.path.join(SOURCE_PATH, \"generated\"))\n",
    "    df_gen[\"filenameid\"] = df_gen[\"filenameid\"].str.replace(\"_transformed_step1\", \"\")\n",
    "    \n",
    "    df_orig = files_to_df(os.path.join(SOURCE_PATH, \"original\"))\n",
    "    df_pairs = df_orig.merge(df_gen, on=\"filenameid\", suffixes=(\"_orig\", \"_gen\"))\n",
    "    assert len(df_pairs) == N_EXPECTED_SAMPLES, f\"Expected {N_EXPECTED_SAMPLES} samples, got {len(df_pairs)}\"\n",
    "    \n",
    "    df_pairs.rename(columns={\"text_orig\": \"clinical_case\", \"text_gen\": \"discharge_summary\"}, inplace=True)\n",
    "    \n",
    "    # 2. Load human evaluation dataset and format it\n",
    "    \n",
    "    # Input: human_eval.csv (From Google Forms)\n",
    "    # Timestamp,Email Address,Original file name (e.g. 36951253),Overall validation [Content Relevance],Overall validation [Information Completeness],\n",
    "    # Overall validation [Clarity and Structure],Overall validation [Content Accuracy],Overall validation [Hallucinations],Overall validation [Impact of Hallucinations],\n",
    "    # Overall validation [Relevance to Practice],Overall validation [Overall Quality],\n",
    "    # Positive highlights: Describe what aspects of the synthetic discharge summaries resemble the best real EHRs? (Empty if nothing remarkable),\n",
    "    # Negative highlights: Which aspects of the synthetic discharge summaries do not resemble well real EHRs? (Empty if nothing remarkable),\n",
    "    # Other Comments: Do you have any other feedback or comment on the generated synthetic discharge summaries or in the original cases? (Empty if nothing remarkable)\n",
    "\n",
    "    df_human = pd.read_csv(os.path.join(SOURCE_PATH, \"human_eval.csv\")).rename(columns={\"Original file name (e.g. 36951253)\": \"filenameid\"}).drop(columns=[\"Email Address\", \"Timestamp\"]).fillna(\"\")\n",
    "    d_score_cols = {\n",
    "                            \"Overall validation [Content Relevance]\": \"Content Relevance\",\n",
    "                            \"Overall validation [Information Completeness]\": \"Information Completeness\",\n",
    "                            \"Overall validation [Clarity and Structure]\": \"Clarity and Structure\",\n",
    "                            \"Overall validation [Content Accuracy]\": \"Content Accuracy\",\n",
    "                            \"Overall validation [Hallucinations]\": \"Hallucinations\",\n",
    "                            \"Overall validation [Impact of Hallucinations]\": \"Impact of Hallucinations\",\n",
    "                            \"Overall validation [Relevance to Practice]\": \"Relevance to Practice\",\n",
    "                            \"Overall validation [Overall Quality]\": \"Overall Quality\",\n",
    "                            \"Positive highlights: Describe what aspects of the synthetic discharge summaries resemble the best real EHRs? (Empty if nothing remarkable)\": \"Positive highlights\",\n",
    "                            \"Negative highlights: Which aspects of the synthetic discharge summaries do not resemble well real EHRs? (Empty if nothing remarkable)\": \"Negative highlights\",\n",
    "                            \"Other Comments: Do you have any other feedback or comment on the generated synthetic discharge summaries or in the original cases? (Empty if nothing remarkable)\": \"Other Comments\"\n",
    "    }\n",
    "\n",
    "    df_human.rename(columns=d_score_cols, inplace=True)\n",
    "    df_human.rename(columns={\"Original file name (e.g. 36951253)\": \"filenameid\"}, inplace=True)\n",
    "    df_human[\"human_score\"] = df_human.drop(columns=[\"filenameid\"]).to_dict(orient=\"records\")\n",
    "\n",
    "    \n",
    "    # Output: df_human\n",
    "    # | filenameid |                  human_score                     |\n",
    "    # | 33857916   | {'Content Relevance': 1, 'Information Complete...|\n",
    "    \n",
    "    # 3. Load automatic evaluation dataset and format it\n",
    "    \n",
    "    # Input: auto_eval.csv (From Google Forms)\n",
    "    # filename,precision,recall,f1,tp,fp,fn,cluster\n",
    "\n",
    "    df_auto = pd.read_csv(os.path.join(SOURCE_PATH, \"auto_eval.csv\")).drop(columns=[\"cluster\"]).rename(columns={\"filename\": \"filenameid\"})\n",
    "    df_auto[\"auto_score\"] = df_auto.drop(columns=[\"filenameid\"]).to_dict(orient=\"records\")\n",
    "    \n",
    "    # Ensure filenameid is string\n",
    "    df_pairs[\"filenameid\"] = df_pairs[\"filenameid\"].map(str)\n",
    "    df_human[\"filenameid\"] = df_human[\"filenameid\"].map(str)\n",
    "    df_auto[\"filenameid\"] = df_auto[\"filenameid\"].map(str)\n",
    "    \n",
    "    # Output: df_auto\n",
    "    # | filenameid |                  auto_score                     |\n",
    "    # | 33857916   | {'precision': 0.5, 'recall': 0.5, 'f1': 0.5,...|\n",
    "    \n",
    "    return df_pairs, df_human, df_auto\n",
    "\n",
    "def select_examples(df_prompt, n=5, seed=42, examples_ids=None):\n",
    "    \"\"\"Select a few examples for few-shot learning.\"\"\"\n",
    "    \n",
    "    if not examples_ids:\n",
    "        example_filenames = df_prompt.sample(n, random_state=seed)[\"filenameid\"].tolist()\n",
    "    else:\n",
    "        example_filenames = df_prompt[df_prompt[\"filenameid\"].isin(examples_ids)]\n",
    "    \n",
    "    log_info(f\"Selected Examples: {example_filenames}\")\n",
    "    \n",
    "    return df_prompt[df_prompt[\"filenameid\"].isin(example_filenames)]\n",
    "\n",
    "def prepare_prompt_data(df_pairs, df_human, df_auto, examples_ids=None):\n",
    "    \"\"\"Merge datasets and prepare prompt inputs.\"\"\"\n",
    "    df_prompt = df_pairs.merge(df_human[[\"filenameid\", \"human_score\"]], on=\"filenameid\").merge(df_auto[[\"filenameid\", \"auto_score\"]], on=\"filenameid\")\n",
    "    if examples_ids:\n",
    "        df_prompt = df_prompt[df_prompt[\"filenameid\"].isin(examples_ids)]\n",
    "    return df_prompt\n",
    "\n",
    "def generate_prompts(df_prompt, guidelines, template, examples):\n",
    "    \"\"\"Generate prompts for LLM processing.\"\"\"\n",
    "    df_prompt[\"prompts\"] = df_prompt.progress_apply(lambda x: Prompt(\n",
    "                                                                        guidelines=guidelines,\n",
    "                                                                        template=template,\n",
    "                                                                        clinical_case=x[\"clinical_case\"],\n",
    "                                                                        discharge_summary=x[\"discharge_summary\"],\n",
    "                                                                        examples=str(examples),\n",
    "                                                                    ).text, axis=1)\n",
    "    return df_prompt\n",
    "\n",
    "def compute_correlations(df_human_preds, df_preds):\n",
    "    \"\"\"Compute Pearson correlation between human and model scores.\"\"\"\n",
    "    return pearsonr(df_human_preds[\"Overall Quality\"], df_preds[\"Overall Quality\"])\n",
    "\n",
    "def plot_correlation_heatmap(df_hm, df_llm, suffixes=(\"_hm\", \"_llm\")):\n",
    "    \"\"\"Plot a heatmap of correlations.\"\"\"\n",
    "    \n",
    "    df_hm_llm_corr = df_hm.merge(df_llm, on=\"filenameid\", suffixes=suffixes)\n",
    "    df_hm_llm_corr = df_hm_llm_corr.select_dtypes(np.number).corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    x_suffix, y_suffix = suffixes[0], suffixes[1]\n",
    "    x_cols = [col for col in df_hm_llm_corr.columns if col.endswith(x_suffix)]\n",
    "    y_cols = [col for col in df_hm_llm_corr.columns if col.endswith(y_suffix)]\n",
    "\n",
    "    corr_matrix = df_hm_llm_corr.loc[x_cols, y_cols]\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1, ax=ax)\n",
    "    ax.set_title(\"Correlation Heatmap: Human vs LLM\")\n",
    "    fig.tight_layout()\n",
    "    # plt.savefig(os.path.join(output_path, \"correlation_heatmap.png\"))   \n",
    "    return fig , corr_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 17:02:21,672 - INFO - Starting evaluation of /gpfs/projects/bsc14/abecerr1/hub/models--Henrychur--MMed-Llama-3-8B-EnIns/snapshots/45936f724a7eabfce59cd8eaed318970db468cad\n",
      "2025-04-03 17:02:21,673 - INFO - \n",
      "2025-04-03 17:02:21,673 - INFO - Loading templates and guidelines from utils/templates/basic\n",
      "2025-04-03 17:02:21,674 - INFO - Loading datasets from output/samples/en/phase_2/\n",
      "2025-04-03 17:02:21,689 - INFO - \n",
      "2025-04-03 17:02:21,689 - INFO - Selecting 0 examples for few-shot learning and generating prompts\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 22869.71it/s]\n",
      "2025-04-03 17:02:21,693 - INFO - \n",
      "2025-04-03 17:02:21,693 - INFO - Starting generation of evaluation results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66076061ade74a9dbc37e9d67eb1bb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /gpfs/projects/bsc14/abecerr1/hub/models--Henrychur--MMed-Llama-3-8B-EnIns/snapshots/45936f724a7eabfce59cd8eaed318970db468cad loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Main execution function.\"\"\"\n",
    "\n",
    "log_info(f\"Starting evaluation of {MODEL_ID}\")\n",
    "log_info(\"\")\n",
    "log_info(f\"Loading templates and guidelines from {TEMPLATES_PATH}\")\n",
    "guidelines = load_file_content(os.path.join(TEMPLATES_PATH, \"guidelines.txt\"))\n",
    "template = load_file_content(os.path.join(TEMPLATES_PATH, \"template.txt\"))\n",
    "example_template = load_file_content(os.path.join(TEMPLATES_PATH,\"example_template.txt\"))\n",
    "system_prompt = load_file_content(os.path.join(TEMPLATES_PATH,\"system.txt\"))\n",
    "\n",
    "log_info(f\"Loading datasets from {SOURCE_PATH}\")\n",
    "df_pairs, df_human, df_auto = load_datasets()\n",
    "df_prompt = prepare_prompt_data(df_pairs, df_human, df_auto)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(f\"Selecting {N_EXAMPLES} examples for few-shot learning and generating prompts\")\n",
    "if N_EXAMPLES > 0:\n",
    "    df_examples = select_examples(df_prompt, n=N_EXAMPLES)\n",
    "    few_shot_examples = df_examples.to_dict(orient=\"records\")\n",
    "    examples = create_examples(few_shot_examples, example_template=example_template)\n",
    "else:\n",
    "    examples = [\"\"]\n",
    "\n",
    "df_prompt = generate_prompts(df_prompt, guidelines, template, examples)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(\"Starting generation of evaluation results\")\n",
    "model = LlamaInstruct(MODEL_ID, device=\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = model.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a case of an adult male patient who presented with chest pain and a positive cardiac biomarker, ST-segment depression on electrocardiogram, and significant (90%) stenosis in left anterior descending artery on coronary angiography. The patient received percutaneous coronary intervention and stent placement for his condition.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from fastchat.model import get_conversation_template\n",
    "import torch\n",
    "\n",
    "# Load local BioMistral model\n",
    "model_path = \"/gpfs/projects/bsc14/abecerr1/hub/models--BioMistral--BioMistral-7B/snapshots/9a11e1ffa817c211cbb52ee1fb312dc6b61b40a5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Use FastChat's conversation template for BioMistral\n",
    "conv = get_conversation_template(\"BioMistral/BioMistral-7B\")  # or \"mistral\" as fallback\n",
    "conv.set_system_message(\"You are a helpful clinical assistant.\")\n",
    "conv.append_message(conv.roles[0], \"Summarize the following clinical case in 2-3 sentences:\\n\\nA 56-year-old male with chest pain and elevated troponin. ECG shows ST depressions in V4-V6. Coronary angiography reveals a 90% LAD stenosis. He was treated with PCI and stent placement.\")\n",
    "conv.append_message(conv.roles[1], None)  # This is the assistant's turn\n",
    "\n",
    "# Build prompt from conversation\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode and isolate assistant response\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = output_text.split(conv.roles[1])[-1].strip()  # Gets the assistant response\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Summarize the following clinical case in 2-3 sentences.\n",
      "\n",
      "### Clinical Case:\n",
      "A 56-year-old male with chest pain and elevated troponin. ECG shows ST depressions in V4-V6. Coronary angiography reveals a 90% LAD stenosis. He was treated with PCI and stent placement.\n",
      "\n",
      "### Summary:\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = (\n",
    "    \"### Instruction:\\n\"\n",
    "    \"Summarize the following clinical case in 2-3 sentences.\\n\\n\"\n",
    "    \"### Clinical Case:\\n\"\n",
    "    \"A 56-year-old male with chest pain and elevated troponin. ECG shows ST depressions in V4-V6. \"\n",
    "    \"Coronary angiography reveals a 90% LAD stenosis. He was treated with PCI and stent placement.\\n\\n\"\n",
    "    \"### Summary:\"\n",
    ")\n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:607: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916ee0d9fc04fbe97fe971d98a292a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3393 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 98\u001b[0m\n\u001b[1;32m     87\u001b[0m discharge_summary \u001b[38;5;241m=\u001b[39m df_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdischarge_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m11\u001b[39m]\n\u001b[1;32m     89\u001b[0m rubric \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriteria\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes the summary focus on clinically relevant information?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore1_description\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe summary largely misses the clinically relevant details.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore5_description\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe summary is entirely focused on clinically relevant information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m }\n\u001b[0;32m---> 98\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_clinical_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclinical_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdischarge_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrubric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "Cell \u001b[0;32mIn[44], line 60\u001b[0m, in \u001b[0;36mevaluate_clinical_summaries\u001b[0;34m(clinical_case, discharge_summary, rubric, output_file)\u001b[0m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m json_candidate \u001b[38;5;241m=\u001b[39m decoded\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    584\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m         position_embeddings,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/gpfs/projects/bsc14/scratch/.conda/msds/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load MedAlpaca model + tokenizer\n",
    "model_path = \"/gpfs/projects/bsc14/abecerr1/hub/models--medalpaca--medalpaca-7b/snapshots/fbb41b75d5a46ba405d496db1083a6f1d3df72a2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Helper function to safely extract JSON\n",
    "def safe_json_extract(text):\n",
    "    match = re.search(r\"\\{[^{}]*\\\"feedback\\\"[^{}]*\\\"score\\\"\\s*:\\s*\\d[^{}]*\\}\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Core evaluation function\n",
    "def evaluate_clinical_summaries(clinical_case, discharge_summary, rubric, output_file=None):\n",
    "    \"\"\"\n",
    "    Uses MedAlpaca to evaluate a clinical discharge summary using rubric.\n",
    "    Returns: dict with 'feedback' and 'score'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a clinical evaluator. Given the clinical case and the generated discharge summary, compare them and respond with a JSON object containing:\n",
    "\n",
    "- \"feedback\": A short explanation (1-3 sentences)\n",
    "- \"score\": An integer from 1 to 5 based on the rubric\n",
    "\n",
    "Only output a valid JSON object. Do not include any other text.\n",
    "\n",
    "If the clinical case is too long, summarize it preserving the most important details.\n",
    "\n",
    "### Evaluation Criterion:\n",
    "{rubric['criteria']}\n",
    "\n",
    "### Scoring Rubric:\n",
    "1 - {rubric['score1_description']}\n",
    "2 - {rubric['score2_description']}\n",
    "3 - {rubric['score3_description']}\n",
    "4 - {rubric['score4_description']}\n",
    "5 - {rubric['score5_description']}\n",
    "\n",
    "### Clinical Case:\n",
    "{clinical_case}\n",
    "\n",
    "### Discharge Summary:\n",
    "{discharge_summary}\n",
    "\n",
    "### JSON Response:\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(3):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        json_candidate = decoded.replace(prompt, \"\").strip()\n",
    "\n",
    "        result = safe_json_extract(json_candidate)\n",
    "        if result:\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    json.dump(result, f, indent=2)\n",
    "            return result\n",
    "\n",
    "        print(f\"⚠️ Attempt {attempt+1}: Failed to extract JSON\\nRaw Output:\\n{json_candidate}\")\n",
    "\n",
    "    return {\"feedback\": \"Unable to parse model output after retries.\", \"score\": None}\n",
    "\n",
    "\n",
    "clinical_case = df_prompt[\"clinical_case\"].iloc[11]\n",
    "discharge_summary = df_prompt[\"discharge_summary\"].iloc[11]\n",
    "\n",
    "rubric = {\n",
    "    \"criteria\": \"Does the summary focus on clinically relevant information?\",\n",
    "    \"score1_description\": \"The summary largely misses the clinically relevant details.\",\n",
    "    \"score2_description\": \"The summary includes only a few clinically relevant details.\",\n",
    "    \"score3_description\": \"The summary covers some relevant information but omits key aspects.\",\n",
    "    \"score4_description\": \"The summary covers most clinically relevant details with minor omissions.\",\n",
    "    \"score5_description\": \"The summary is entirely focused on clinically relevant information.\"\n",
    "}\n",
    "\n",
    "out = evaluate_clinical_summaries(clinical_case, discharge_summary, rubric)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def evaluate_clinical_summaries(clinical_case, discharge_summary, rubric, output_file=None):\n",
    "    \"\"\"\n",
    "    Evaluate clinical case summaries against discharge summaries.\n",
    "    \n",
    "    Args:\n",
    "        clinical_cases (list): List of clinical case texts.\n",
    "        discharge_summaries (list): List of discharge summary texts.\n",
    "        output_file (str): Path to save the output dictionary.\n",
    "        generate_score (callable): Function to generate scores using the AI model.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "\n",
    "    \n",
    "    instruction = \"\"\"\n",
    "    One of the main bottlenecks for the development of clinical NLP resources if the lack of access to clinical records due to data privacy issues. This is particularly true for developments beyond English, as most of the accessible anonymized clinical record datasets are only available for this language.\n",
    "    To examine if clinical case report publications could potentially be considered as a data source to generate synthetic clinical discharge summaries by means of generative AI solutions, prompt instructions combined with automatic clinical were applied.\n",
    "    This structured summary has the purpose to systematically characterize the clinical language characteristics of synthetic discharge summaries.\n",
    "    Each discharge summary was assessed for a predefined set of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    ABSOLUTE_PROMPT = f\"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"(write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{discharge_summary}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{clinical_case}\n",
    "\n",
    "###Score Rubrics:\n",
    "{rubric}\n",
    "\n",
    "###Feedback: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ABS_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": ABSOLUTE_PROMPT}]\n",
    "        # response = generate_score(prompt)\n",
    "    \n",
    "    outputs = pipe(\n",
    "                    messages,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.01,\n",
    "                    eos_token_id=terminators,\n",
    "                    pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "\n",
    "    # try:\n",
    "    #     return json.loads(outputs[0][\"generated_text\"][-1][\"content\"])\n",
    "    # except:\n",
    "    #     print(outputs[0][\"generated_text\"][-1][\"content\"])\n",
    "    #     return None\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "      \n",
    "clinical_case = df_prompt[\"clinical_case\"].iloc[0]\n",
    "discharge_summary = df_prompt[\"discharge_summary\"].iloc[0]\n",
    "rubric = {\n",
    "        \"criteria\": \"Does the summary focus on clinically relevant information?\",\n",
    "        \"score1_description\": \"The summary largely misses the clinically relevant details.\",\n",
    "        \"score2_description\": \"The summary includes only a few clinically relevant details.\",\n",
    "        \"score3_description\": \"The summary covers some relevant information but omits key aspects.\",\n",
    "        \"score4_description\": \"The summary covers most clinically relevant details with minor omissions.\",\n",
    "        \"score5_description\": \"The summary is entirely focused on clinically relevant information.\"\n",
    "    }\n",
    "\n",
    "out = evaluate_clinical_summaries(clinical_case, discharge_summary, rubric, output_file=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Discharge Summary**\\n\\n**Patient Information:**\\n- Name: [Patient Name]\\n- Age: 68 years\\n- Gender: Male\\n- Admission Date: [Admission Date]\\n- Discharge Date: [Discharge Date]\\n\\n**Medical History:**\\n- Coronary artery disease (CABG and stenting)\\n- Ischemic cardiomyopathy leading to ACC/AHA stage D chronic systolic heart failure (IABP and LVAD placement)\\n- Type 2 diabetes mellitus\\n- Hypertension\\n- Chronic kidney disease\\n- Obstructive sleep apnea\\n- History of smoking\\n\\n**Surgical History:**\\n- Orthotopic heart transplantation (February 2020)\\n\\n**Admission Details:**\\n- Readmission Date: [Readmission Date]\\n- Chief Complaints: Non-productive cough, non-bloody diarrhea for one week, fevers (max 38.6 °C), sternal wound discharge.\\n- Family History: Similar respiratory symptoms and fevers reported among family members.\\n\\n**Clinical Findings:**\\n- Vital Signs: Afebrile, hemodynamically stable\\n- Physical Examination: Dehiscence of lower sternal incision with minimal foul-smelling serosanguinous drainage; otherwise unremarkable respiratory and cardiovascular examination.\\n\\n**Laboratory and Imaging Studies:**\\n- WBC: 7.820 × 10^9/L (N: 3.8–10.5 × 10^9/L)\\n- Lymphocyte Count: 0.2 × 10^9/L (N: 1.0–3.0 × 10^9/L)\\n- Elevated inflammatory markers: CRP 1612.38 nmol/L (N: 0–3.81 nmol/L), Procalcitonin 0.72 μg/L (N: 0.02–0.10 μg/L), LDH 6083.33 nmol/(s•L) (N: 833.33–4033.33 nmol/(s•L)), Ferritin 1.15 nmol/L (N: 0.07–0.90 nmol/L), D-dimer 1194 μg/L DDU (N: ≤ 229 μg/L DDU).\\n- CT Chest/Abdomen/Pelvis: Ground-glass opacities in bilateral lower lobes; no fluid collections at sternotomy site.\\n- Nasopharyngeal PCR: Positive for SARS-CoV-2.\\n\\n**Hospital Course:**\\n- Infectious diseases consulted; not eligible for interleukin inhibitors due to immunosuppressive therapy.\\n- Initiated on IV vancomycin and meropenem for suspected sternal wound infection; bedside sternal wound debridement and vacuum therapy performed.\\n- Cultures: Morganella morganii and Enterobacter cloacae from sternal wound; blood cultures positive for Morganella.\\n- Complications included encephalopathy, respiratory failure requiring intubation, acute kidney injury necessitating hemodialysis, and recurrent infections.\\n- Sternal wound reconstruction with pectoralis major muscle flaps; MMF discontinued, cyclosporine initiated.\\n- Developed fungal infection (Rhizopus microsporus) requiring surgical intervention and antifungal therapy.\\n- Hospital course complicated by multiple infections, septic shock, and ultimately, death on day 175 of hospitalization.\\n\\n**Outcome:**\\n- Unfortunately, the patient succumbed to complications related to his underlying conditions and infections.\\n\\n**Plan:**\\n- [Further recommendations or follow-up plans if applicable]\\n\\n**Prepared by:**\\n[Provider Name]\\n[Title]\\n[Date]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discharge_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable. (request to http://localhost:8080/api/kernels?1743696952446 failed, reason: ).)."
     ]
    }
   ],
   "source": [
    "\n",
    "# df_prompt = df_prompt.sample(3) # For testing\n",
    "df_prompt[\"generation\"] = df_prompt.progress_apply(lambda x: evaluate_clinical_summaries(clinical_case=x[\"clinical_case\"],\n",
    "    discharge_summary=x[\"discharge_summary\"],\n",
    "    rubric={\n",
    "        \"criteria\": \"Does the summary focus on clinically relevant information?\",\n",
    "        \"score1_description\": \"The summary largely misses the clinically relevant details.\",\n",
    "        \"score2_description\": \"The summary includes only a few clinically relevant details.\",\n",
    "        \"score3_description\": \"The summary covers some relevant information but omits key aspects.\",\n",
    "        \"score4_description\": \"The summary covers most clinically relevant details with minor omissions.\",\n",
    "        \"score5_description\": \"The summary is entirely focused on clinically relevant information.\"\n",
    "    }\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenameid</th>\n",
       "      <th>clinical_case</th>\n",
       "      <th>discharge_summary</th>\n",
       "      <th>human_score</th>\n",
       "      <th>auto_score</th>\n",
       "      <th>prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31557359</td>\n",
       "      <td>A 61‐year‐old man, diabetic and hypertensive, ...</td>\n",
       "      <td>**Discharge Summary**\\n\\n**Patient Information...</td>\n",
       "      <td>{'Content Relevance': 5, 'Information Complete...</td>\n",
       "      <td>{'precision': 0.599999940000006, 'recall': 0.6...</td>\n",
       "      <td>Look at these guidelines carefully, i have als...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filenameid                                      clinical_case  \\\n",
       "7   31557359  A 61‐year‐old man, diabetic and hypertensive, ...   \n",
       "\n",
       "                                   discharge_summary  \\\n",
       "7  **Discharge Summary**\\n\\n**Patient Information...   \n",
       "\n",
       "                                         human_score  \\\n",
       "7  {'Content Relevance': 5, 'Information Complete...   \n",
       "\n",
       "                                          auto_score  \\\n",
       "7  {'precision': 0.599999940000006, 'recall': 0.6...   \n",
       "\n",
       "                                             prompts  \n",
       "7  Look at these guidelines carefully, i have als...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prompt.iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback': 'The summary focuses on clinically relevant information, though minor details about sensing and pacing rhythms would be useful for optimizing therapy',\n",
       " 'score': 4}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prompt[\"generation\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_human_preds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39massign(filenameid\u001b[38;5;241m=\u001b[39mdf_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenameid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 2\u001b[0m df_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prompt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39massign(filenameid\u001b[38;5;241m=\u001b[39mdf_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenameid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      4\u001b[0m eval_metric \u001b[38;5;241m=\u001b[39m compute_correlations(df_human_preds, df_preds)\n\u001b[1;32m      5\u001b[0m log_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m--> 837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_dict_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m    839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:917\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    915\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[0;32m--> 917\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_unique_multiple_list_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(pre_cols)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n",
      "File \u001b[0;32mlib.pyx:367\u001b[0m, in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03mConvert list of dicts to numpy arrays\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03mcolumns : Index\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    917\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[38;5;241m=\u001b[39msort)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "df_human_preds = pd.DataFrame(df_prompt[\"human_score\"].tolist()).assign(filenameid=df_prompt[\"filenameid\"].values)\n",
    "df_preds = pd.DataFrame(df_prompt[\"generation\"].tolist()).assign(filenameid=df_prompt[\"filenameid\"].values)\n",
    "\n",
    "eval_metric = compute_correlations(df_human_preds, df_preds)\n",
    "log_info(f\"Evaluation Metric: {eval_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, df_hm_llm_corr = plot_correlation_heatmap(df_human_preds, df_preds)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(f\"Saving results to {OUTPUT_PATH}\")\n",
    "    \n",
    "fig.savefig(os.path.join(OUTPUT_PATH, \"correlation_heatmap.png\"))\n",
    "df_human_preds.to_csv(os.path.join(OUTPUT_PATH, \"human_predictions.csv\"), index=False)\n",
    "df_preds.to_csv(os.path.join(OUTPUT_PATH, \"llm_predictions.csv\"), index=False)\n",
    "df_hm_llm_corr.to_csv(os.path.join(OUTPUT_PATH, \"correlation_matrix.csv\"), index=True)\n",
    "df_prompt.to_csv(os.path.join(OUTPUT_PATH, \"prompt_data.csv\"), index=False)\n",
    "\n",
    "\n",
    "if N_EXAMPLES > 0:\n",
    "    df_examples_human = pd.DataFrame(df_examples[\"human_score\"].tolist()).assign(filenameid=df_examples[\"filenameid\"].values)\n",
    "    df_examples_preds = df_prompt[df_prompt[\"filenameid\"].isin(df_examples[\"filenameid\"].values)]\n",
    "    df_examples_preds = pd.DataFrame(df_examples_preds[\"generation\"].tolist()).assign(filenameid=df_examples_preds[\"filenameid\"].values)\n",
    "    df_examples_auto = pd.DataFrame(df_examples[\"auto_score\"].tolist()).assign(filenameid=df_examples[\"filenameid\"].values)\n",
    "    \n",
    "    df_examples_human.to_csv(os.path.join(OUTPUT_PATH, \"examples_human_eval.csv\"), index=False)\n",
    "    df_examples_preds.to_csv(os.path.join(OUTPUT_PATH, \"examples_predictions.csv\"), index=False)\n",
    "    df_examples_auto.to_csv(os.path.join(OUTPUT_PATH, \"examples_auto_eval.csv\"), index=False)\n",
    "    \n",
    "log_info(f\"Results saved to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
