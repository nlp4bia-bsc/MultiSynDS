{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 10:57:47,372 - INFO - Configuration Settings:\n",
      "2025-04-05 10:57:47,373 - INFO - N_EXPECTED_SAMPLES: 35\n",
      "2025-04-05 10:57:47,373 - INFO - N_EXAMPLES: 7\n",
      "2025-04-05 10:57:47,373 - INFO - MODEL_ID: /gpfs/projects/bsc14/abecerr1/hub/models--Henrychur--MMed-Llama-3-8B-EnIns/snapshots/45936f724a7eabfce59cd8eaed318970db468cad\n",
      "2025-04-05 10:57:47,374 - INFO - SOURCE_PATH: output/samples/en/phase_2/\n",
      "2025-04-05 10:57:47,374 - INFO - TEMPLATES_PATH: utils/templates/basic\n",
      "2025-04-05 10:57:47,374 - INFO - OUTPUT_PATH: output/evaluation/MMed-Llama-3-8B-EnIns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append(\".\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from src.data import files_to_df, Prompt, create_examples\n",
    "from src.generative_models import LlamaInstruct\n",
    "from src.generate import safe_generate\n",
    "from src.utils import setup_logger, log_info, path_with_datetime, load_config, log_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(config_path=\"scripts/MMed-Llama-3-8B-EnIns/config.yaml\")\n",
    "\n",
    "N_EXPECTED_SAMPLES = config[\"N_EXPECTED_SAMPLES\"]\n",
    "# N_EXAMPLES = config[\"N_EXAMPLES\"]\n",
    "N_EXAMPLES = 0\n",
    "MODEL_ID = config[\"MODEL_ID\"]\n",
    "SOURCE_PATH = config[\"SOURCE_PATH\"]\n",
    "TEMPLATES_PATH = config[\"TEMPLATES_PATH\"]\n",
    "OUTPUT_PATH = path_with_datetime(config[\"OUTPUT_PATH\"])\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# write config to file in OUTPUT_PATH\n",
    "with open(os.path.join(OUTPUT_PATH, \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "        \n",
    "# Setup logger\n",
    "setup_logger(os.path.join(OUTPUT_PATH, \"app.log\"))\n",
    "log_config(config)\n",
    "\n",
    "def load_file_content(filepath):\n",
    "    \"\"\"Safely load text file content.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return file.read().strip()  # Strip trailing spaces\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load generated, original, human evaluation, and automatic evaluation datasets.\n",
    "    \n",
    "    Returns:\n",
    "        df_pairs: pd.DataFrame, pairs of generated and original samples\n",
    "        df_human: pd.DataFrame, human evaluation scores\n",
    "        df_auto: pd.DataFrame, automatic evaluation scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load generated and original datasets and format it\n",
    "    df_gen = files_to_df(os.path.join(SOURCE_PATH, \"generated\"))\n",
    "    df_gen[\"filenameid\"] = df_gen[\"filenameid\"].str.replace(\"_transformed_step1\", \"\")\n",
    "    \n",
    "    df_orig = files_to_df(os.path.join(SOURCE_PATH, \"original\"))\n",
    "    df_pairs = df_orig.merge(df_gen, on=\"filenameid\", suffixes=(\"_orig\", \"_gen\"))\n",
    "    assert len(df_pairs) == N_EXPECTED_SAMPLES, f\"Expected {N_EXPECTED_SAMPLES} samples, got {len(df_pairs)}\"\n",
    "    \n",
    "    df_pairs.rename(columns={\"text_orig\": \"clinical_case\", \"text_gen\": \"discharge_summary\"}, inplace=True)\n",
    "    \n",
    "    # 2. Load human evaluation dataset and format it\n",
    "    \n",
    "    # Input: human_eval.csv (From Google Forms)\n",
    "    # Timestamp,Email Address,Original file name (e.g. 36951253),Overall validation [Content Relevance],Overall validation [Information Completeness],\n",
    "    # Overall validation [Clarity and Structure],Overall validation [Content Accuracy],Overall validation [Hallucinations],Overall validation [Impact of Hallucinations],\n",
    "    # Overall validation [Relevance to Practice],Overall validation [Overall Quality],\n",
    "    # Positive highlights: Describe what aspects of the synthetic discharge summaries resemble the best real EHRs? (Empty if nothing remarkable),\n",
    "    # Negative highlights: Which aspects of the synthetic discharge summaries do not resemble well real EHRs? (Empty if nothing remarkable),\n",
    "    # Other Comments: Do you have any other feedback or comment on the generated synthetic discharge summaries or in the original cases? (Empty if nothing remarkable)\n",
    "\n",
    "    df_human = pd.read_csv(os.path.join(SOURCE_PATH, \"human_eval.csv\")).rename(columns={\"Original file name (e.g. 36951253)\": \"filenameid\"}).drop(columns=[\"Email Address\", \"Timestamp\"]).fillna(\"\")\n",
    "    d_score_cols = {\n",
    "                            \"Overall validation [Content Relevance]\": \"Content Relevance\",\n",
    "                            \"Overall validation [Information Completeness]\": \"Information Completeness\",\n",
    "                            \"Overall validation [Clarity and Structure]\": \"Clarity and Structure\",\n",
    "                            \"Overall validation [Content Accuracy]\": \"Content Accuracy\",\n",
    "                            \"Overall validation [Hallucinations]\": \"Hallucinations\",\n",
    "                            \"Overall validation [Impact of Hallucinations]\": \"Impact of Hallucinations\",\n",
    "                            \"Overall validation [Relevance to Practice]\": \"Relevance to Practice\",\n",
    "                            \"Overall validation [Overall Quality]\": \"Overall Quality\",\n",
    "                            \"Positive highlights: Describe what aspects of the synthetic discharge summaries resemble the best real EHRs? (Empty if nothing remarkable)\": \"Positive highlights\",\n",
    "                            \"Negative highlights: Which aspects of the synthetic discharge summaries do not resemble well real EHRs? (Empty if nothing remarkable)\": \"Negative highlights\",\n",
    "                            \"Other Comments: Do you have any other feedback or comment on the generated synthetic discharge summaries or in the original cases? (Empty if nothing remarkable)\": \"Other Comments\"\n",
    "    }\n",
    "\n",
    "    df_human.rename(columns=d_score_cols, inplace=True)\n",
    "    df_human.rename(columns={\"Original file name (e.g. 36951253)\": \"filenameid\"}, inplace=True)\n",
    "    df_human[\"human_score\"] = df_human.drop(columns=[\"filenameid\"]).to_dict(orient=\"records\")\n",
    "\n",
    "    \n",
    "    # Output: df_human\n",
    "    # | filenameid |                  human_score                     |\n",
    "    # | 33857916   | {'Content Relevance': 1, 'Information Complete...|\n",
    "    \n",
    "    # 3. Load automatic evaluation dataset and format it\n",
    "    \n",
    "    # Input: auto_eval.csv (From Google Forms)\n",
    "    # filename,precision,recall,f1,tp,fp,fn,cluster\n",
    "\n",
    "    df_auto = pd.read_csv(os.path.join(SOURCE_PATH, \"auto_eval.csv\")).drop(columns=[\"cluster\"]).rename(columns={\"filename\": \"filenameid\"})\n",
    "    df_auto[\"auto_score\"] = df_auto.drop(columns=[\"filenameid\"]).to_dict(orient=\"records\")\n",
    "    \n",
    "    # Ensure filenameid is string\n",
    "    df_pairs[\"filenameid\"] = df_pairs[\"filenameid\"].map(str)\n",
    "    df_human[\"filenameid\"] = df_human[\"filenameid\"].map(str)\n",
    "    df_auto[\"filenameid\"] = df_auto[\"filenameid\"].map(str)\n",
    "    \n",
    "    # Output: df_auto\n",
    "    # | filenameid |                  auto_score                     |\n",
    "    # | 33857916   | {'precision': 0.5, 'recall': 0.5, 'f1': 0.5,...|\n",
    "    \n",
    "    return df_pairs, df_human, df_auto\n",
    "\n",
    "def select_examples(df_prompt, n=5, seed=42, examples_ids=None):\n",
    "    \"\"\"Select a few examples for few-shot learning.\"\"\"\n",
    "    \n",
    "    if not examples_ids:\n",
    "        example_filenames = df_prompt.sample(n, random_state=seed)[\"filenameid\"].tolist()\n",
    "    else:\n",
    "        example_filenames = df_prompt[df_prompt[\"filenameid\"].isin(examples_ids)]\n",
    "    \n",
    "    log_info(f\"Selected Examples: {example_filenames}\")\n",
    "    \n",
    "    return df_prompt[df_prompt[\"filenameid\"].isin(example_filenames)]\n",
    "\n",
    "def prepare_prompt_data(df_pairs, df_human, df_auto, examples_ids=None):\n",
    "    \"\"\"Merge datasets and prepare prompt inputs.\"\"\"\n",
    "    df_prompt = df_pairs.merge(df_human[[\"filenameid\", \"human_score\"]], on=\"filenameid\").merge(df_auto[[\"filenameid\", \"auto_score\"]], on=\"filenameid\")\n",
    "    if examples_ids:\n",
    "        df_prompt = df_prompt[df_prompt[\"filenameid\"].isin(examples_ids)]\n",
    "    return df_prompt\n",
    "\n",
    "def generate_prompts(df_prompt, guidelines, template, examples):\n",
    "    \"\"\"Generate prompts for LLM processing.\"\"\"\n",
    "    df_prompt[\"prompts\"] = df_prompt.progress_apply(lambda x: Prompt(\n",
    "                                                                        guidelines=guidelines,\n",
    "                                                                        template=template,\n",
    "                                                                        clinical_case=x[\"clinical_case\"],\n",
    "                                                                        discharge_summary=x[\"discharge_summary\"],\n",
    "                                                                        examples=str(examples),\n",
    "                                                                    ).text, axis=1)\n",
    "    return df_prompt\n",
    "\n",
    "def compute_correlations(df_human_preds, df_preds):\n",
    "    \"\"\"Compute Pearson correlation between human and model scores.\"\"\"\n",
    "    return pearsonr(df_human_preds[\"Overall Quality\"], df_preds[\"Overall Quality\"])\n",
    "\n",
    "def plot_correlation_heatmap(df_hm, df_llm, suffixes=(\"_hm\", \"_llm\")):\n",
    "    \"\"\"Plot a heatmap of correlations.\"\"\"\n",
    "    \n",
    "    df_hm_llm_corr = df_hm.merge(df_llm, on=\"filenameid\", suffixes=suffixes)\n",
    "    df_hm_llm_corr = df_hm_llm_corr.select_dtypes(np.number).corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    x_suffix, y_suffix = suffixes[0], suffixes[1]\n",
    "    x_cols = [col for col in df_hm_llm_corr.columns if col.endswith(x_suffix)]\n",
    "    y_cols = [col for col in df_hm_llm_corr.columns if col.endswith(y_suffix)]\n",
    "\n",
    "    corr_matrix = df_hm_llm_corr.loc[x_cols, y_cols]\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1, ax=ax)\n",
    "    ax.set_title(\"Correlation Heatmap: Human vs LLM\")\n",
    "    fig.tight_layout()\n",
    "    # plt.savefig(os.path.join(output_path, \"correlation_heatmap.png\"))   \n",
    "    return fig , corr_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 10:57:47,465 - INFO - Starting evaluation of /gpfs/projects/bsc14/abecerr1/hub/models--Henrychur--MMed-Llama-3-8B-EnIns/snapshots/45936f724a7eabfce59cd8eaed318970db468cad\n",
      "2025-04-05 10:57:47,466 - INFO - \n",
      "2025-04-05 10:57:47,466 - INFO - Loading templates and guidelines from utils/templates/basic\n",
      "2025-04-05 10:57:47,467 - INFO - Loading datasets from output/samples/en/phase_2/\n",
      "2025-04-05 10:57:47,481 - INFO - \n",
      "2025-04-05 10:57:47,481 - INFO - Selecting 0 examples for few-shot learning and generating prompts\n",
      "100%|██████████| 15/15 [00:00<00:00, 25987.01it/s]\n",
      "2025-04-05 10:57:47,484 - INFO - \n",
      "2025-04-05 10:57:47,484 - INFO - Starting generation of evaluation results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Main execution function.\"\"\"\n",
    "\n",
    "log_info(f\"Starting evaluation of {MODEL_ID}\")\n",
    "log_info(\"\")\n",
    "log_info(f\"Loading templates and guidelines from {TEMPLATES_PATH}\")\n",
    "guidelines = load_file_content(os.path.join(TEMPLATES_PATH, \"guidelines.txt\"))\n",
    "template = load_file_content(os.path.join(TEMPLATES_PATH, \"template.txt\"))\n",
    "example_template = load_file_content(os.path.join(TEMPLATES_PATH,\"example_template.txt\"))\n",
    "system_prompt = load_file_content(os.path.join(TEMPLATES_PATH,\"system.txt\"))\n",
    "\n",
    "log_info(f\"Loading datasets from {SOURCE_PATH}\")\n",
    "df_pairs, df_human, df_auto = load_datasets()\n",
    "df_prompt = prepare_prompt_data(df_pairs, df_human, df_auto)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(f\"Selecting {N_EXAMPLES} examples for few-shot learning and generating prompts\")\n",
    "if N_EXAMPLES > 0:\n",
    "    df_examples = select_examples(df_prompt, n=N_EXAMPLES)\n",
    "    few_shot_examples = df_examples.to_dict(orient=\"records\")\n",
    "    examples = create_examples(few_shot_examples, example_template=example_template)\n",
    "else:\n",
    "    examples = [\"\"]\n",
    "\n",
    "df_prompt = generate_prompts(df_prompt, guidelines, template, examples)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(\"Starting generation of evaluation results\")\n",
    "# model = LlamaInstruct(MODEL_ID, device=\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417becfa9c20465fa9a554f08e15f38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load MedAlpaca model + tokenizer\n",
    "model_path = \"/gpfs/projects/bsc14/abecerr1/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feedback': \"The discharge summary effectively conveys the patient's complex clinical scenario, highlighting the critical role of orthotopic heart transplantation and its associated complications. However, there is a need for more concise language and better organization to facilitate clear understanding by non-experts.\", 'score': 4}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper function to safely extract JSON\n",
    "def safe_json_extract(text):\n",
    "    match = re.search(r\"\\{[^{}]*\\\"feedback\\\"[^{}]*\\\"score\\\"\\s*:\\s*\\d[^{}]*\\}\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Core evaluation function\n",
    "def evaluate_clinical_summaries(clinical_case, discharge_summary, rubric, output_file=None):\n",
    "    \"\"\"\n",
    "    Uses MedAlpaca to evaluate a clinical discharge summary using rubric.\n",
    "    Returns: dict with 'feedback' and 'score'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a clinical evaluator. Given the clinical case and the generated discharge summary, compare them and respond with a JSON object containing:\n",
    "\n",
    "- \"feedback\": A short explanation (1-3 sentences)\n",
    "- \"score\": An integer from 1 to 5 based on the rubric\n",
    "\n",
    "Only output a valid JSON object. Do not include any other text.\n",
    "\n",
    "If the clinical case is too long, summarize it preserving the most important details.\n",
    "\n",
    "### Evaluation Criterion:\n",
    "{rubric['criteria']}\n",
    "\n",
    "### Scoring Rubric:\n",
    "1 - {rubric['score1_description']}\n",
    "2 - {rubric['score2_description']}\n",
    "3 - {rubric['score3_description']}\n",
    "4 - {rubric['score4_description']}\n",
    "5 - {rubric['score5_description']}\n",
    "\n",
    "### Clinical Case:\n",
    "{clinical_case}\n",
    "\n",
    "### Discharge Summary:\n",
    "{discharge_summary}\n",
    "\n",
    "### JSON Response:\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(3):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        json_candidate = decoded.replace(prompt, \"\").strip()\n",
    "\n",
    "        result = safe_json_extract(json_candidate)\n",
    "        if result:\n",
    "            if output_file:\n",
    "                with open(output_file, \"w\") as f:\n",
    "                    json.dump(result, f, indent=2)\n",
    "            return result\n",
    "\n",
    "        print(f\"⚠️ Attempt {attempt+1}: Failed to extract JSON\\nRaw Output:\\n{json_candidate}\")\n",
    "\n",
    "    return {\"feedback\": \"Unable to parse model output after retries.\", \"score\": None}\n",
    "\n",
    "\n",
    "clinical_case = df_prompt[\"clinical_case\"].iloc[11]\n",
    "discharge_summary = df_prompt[\"discharge_summary\"].iloc[11]\n",
    "\n",
    "rubric = {\n",
    "    \"criteria\": \"Does the summary focus on clinically relevant information?\",\n",
    "    \"score1_description\": \"The summary largely misses the clinically relevant details.\",\n",
    "    \"score2_description\": \"The summary includes only a few clinically relevant details.\",\n",
    "    \"score3_description\": \"The summary covers some relevant information but omits key aspects.\",\n",
    "    \"score4_description\": \"The summary covers most clinically relevant details with minor omissions.\",\n",
    "    \"score5_description\": \"The summary is entirely focused on clinically relevant information.\"\n",
    "}\n",
    "\n",
    "out = evaluate_clinical_summaries(clinical_case, discharge_summary, rubric)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 13%|█▎        | 2/15 [00:01<00:09,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 20%|██        | 3/15 [00:01<00:07,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 27%|██▋       | 4/15 [00:05<00:20,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 33%|███▎      | 5/15 [00:07<00:17,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 40%|████      | 6/15 [00:08<00:13,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 47%|████▋     | 7/15 [00:09<00:10,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 53%|█████▎    | 8/15 [00:11<00:11,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 60%|██████    | 9/15 [00:13<00:09,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 67%|██████▋   | 10/15 [00:13<00:06,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 73%|███████▎  | 11/15 [00:15<00:05,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 80%|████████  | 12/15 [00:16<00:03,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 87%|████████▋ | 13/15 [00:17<00:02,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 93%|█████████▎| 14/15 [00:17<00:00,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "100%|██████████| 15/15 [00:19<00:00,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "100%|██████████| 15/15 [00:21<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_prompt[\"generated\"] = df_prompt.progress_apply(\n",
    "    lambda x: evaluate_clinical_summaries(\n",
    "        clinical_case=x[\"clinical_case\"],\n",
    "        discharge_summary=x[\"discharge_summary\"],\n",
    "        rubric=rubric\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_llm = df_prompt[[\"filenameid\", \"generated\"]].rename(columns={\"generated\": \"llm_score\"})\n",
    "df_llm[\"llm_score\"] = df_llm[\"llm_score\"].apply(lambda x: x if isinstance(x, dict) else json.loads(x))\n",
    "score_cols = df_llm[\"llm_score\"].apply(lambda x: pd.Series(x))\n",
    "df_llm = pd.concat([df_llm[\"filenameid\"], score_cols], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenameid</th>\n",
       "      <th>feedback</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29617510</td>\n",
       "      <td>The summary accurately captures the primary di...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31204375</td>\n",
       "      <td>The discharge summary effectively communicates...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30072860_2</td>\n",
       "      <td>The summary effectively captures the core deta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31056078</td>\n",
       "      <td>The discharge summary effectively summarizes t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31486515</td>\n",
       "      <td>The discharge summary effectively conveys the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31512669</td>\n",
       "      <td>The discharge summary adequately covers the ma...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32997782</td>\n",
       "      <td>The discharge summary effectively summarizes t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31557359</td>\n",
       "      <td>The discharge summary effectively conveys the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26989133</td>\n",
       "      <td>The discharge summary effectively conveys crit...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31049155</td>\n",
       "      <td>The summary provides an overview of the patien...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31378248</td>\n",
       "      <td>Summary effectively captures the patient's chi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33857916</td>\n",
       "      <td>The discharge summary provides crucial informa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34530632</td>\n",
       "      <td>The summary effectively captures the essential...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30785587</td>\n",
       "      <td>The discharge summary effectively summarizes t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30930460</td>\n",
       "      <td>The summary provides detailed information abou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filenameid                                           feedback  score\n",
       "0     29617510  The summary accurately captures the primary di...      4\n",
       "1     31204375  The discharge summary effectively communicates...      5\n",
       "2   30072860_2  The summary effectively captures the core deta...      4\n",
       "3     31056078  The discharge summary effectively summarizes t...      4\n",
       "4     31486515  The discharge summary effectively conveys the ...      4\n",
       "5     31512669  The discharge summary adequately covers the ma...      4\n",
       "6     32997782  The discharge summary effectively summarizes t...      4\n",
       "7     31557359  The discharge summary effectively conveys the ...      4\n",
       "8     26989133  The discharge summary effectively conveys crit...      4\n",
       "9     31049155  The summary provides an overview of the patien...      4\n",
       "10    31378248  Summary effectively captures the patient's chi...      4\n",
       "11    33857916  The discharge summary provides crucial informa...      4\n",
       "12    34530632  The summary effectively captures the essential...      4\n",
       "13    30785587  The discharge summary effectively summarizes t...      4\n",
       "14    30930460  The summary provides detailed information abou...      4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm = df_prompt[[\"filenameid\", \"generated\"]].rename(columns={\"generated\": \"llm_score\"})\n",
    "df_llm[\"llm_score\"] = df_llm[\"llm_score\"].apply(lambda x: x if isinstance(x, dict) else json.loads(x))\n",
    "score_cols = df_llm[\"llm_score\"].apply(lambda x: pd.Series(x))\n",
    "df_llm = pd.concat([df_llm[\"filenameid\"], score_cols], axis=1)\n",
    "df_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The summary accurately captures the primary diagnosis and key interventions, highlighting the successful percutaneous closure of the PVL and subsequent hemodynamic stability. However, minor details such as specific medication lists and follow-up appointment schedules could be included for completeness.',\n",
       "       \"The discharge summary effectively communicates the patient's diagnosis, treatment, and follow-up instructions, focusing on the clinically relevant details.\",\n",
       "       \"The summary effectively captures the core details of the clinical scenario, including the patient's diagnosis, interventions, and outcomes. However, some minor omissions regarding specific test results and medications are present.\",\n",
       "       \"The discharge summary effectively summarizes the patient's clinical history, diagnosis, and treatment plan, focusing on clinically relevant information. However, there could be more detail about the patient's symptoms and laboratory results.\",\n",
       "       \"The discharge summary effectively conveys the main points of the patient's hospitalization, including diagnosis, treatment, and outcomes. However, the summary could benefit from more detailed explanations of the underlying causes of the patient's conditions and the rationale behind specific treatments.\",\n",
       "       \"The discharge summary adequately covers the major points of the clinical case, including the patient's presentation, diagnosis, treatment, and outcome. However, the summary could benefit from more detail regarding the patient's laboratory results, specific medication regimens, and a clearer discussion of the rationale behind the chosen management strategies.\",\n",
       "       \"The discharge summary effectively summarizes the patient's medical condition and treatment, highlighting the development of severe complications following the procedure. However, it could benefit from more detail on the patient's pre-existing conditions, laboratory results, and the implications of the elevated TGF-β1 levels.\",\n",
       "       \"The discharge summary effectively conveys the patient's significant clinical history, including the presence of a left bundle branch block and recent changes in QRS morphology upon sensing versus pacing. However, the summary could benefit from clearer connections between these findings and their implications for managing heart failure and optimizing pacing therapy.\",\n",
       "       \"The discharge summary effectively conveys critical information about the patient's diagnosis, treatment, and outcomes, but could benefit from more detail on the nuances of the patient's complex condition.\",\n",
       "       \"The summary provides an overview of the patient's condition, procedures, and outcomes but could benefit from more detail on the specifics of the anatomical anomalies and procedural steps.\",\n",
       "       \"Summary effectively captures the patient's chief complaint, history of present illness, and diagnosis of heart failure with reduced ejection fraction (HFrEF). However, the section on treatment could benefit from more detail on the specific medications and dosages, as well as the rationale behind the choice of treatments.\",\n",
       "       \"The discharge summary provides crucial information about the patient's clinical course, including the diagnosis of COVID-19, surgical interventions, and complications such as sternal wound infections and fungal infections. However, it could benefit from more detail regarding the patient's current medication regimen and specific treatment plans for each condition.\",\n",
       "       \"The summary effectively captures the essential elements of the patient's clinical presentation and management.\",\n",
       "       \"The discharge summary effectively summarizes the clinical presentation, diagnostic findings, and treatment outcomes, but could benefit from a clearer connection to the underlying causes of the patient's condition.\",\n",
       "       \"The summary provides detailed information about the patient's condition, medical history, and treatments, highlighting the progression of his vasculitis and subsequent cardiac complications. However, it could benefit from a clearer summary of the patient's current clinical status.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_llm[\"feedback\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable. (request to http://localhost:8080/api/kernels?1743696952446 failed, reason: ).)."
     ]
    }
   ],
   "source": [
    "\n",
    "# df_prompt = df_prompt.sample(3) # For testing\n",
    "df_prompt[\"generation\"] = df_prompt.progress_apply(lambda x: evaluate_clinical_summaries(clinical_case=x[\"clinical_case\"],\n",
    "    discharge_summary=x[\"discharge_summary\"],\n",
    "    rubric={\n",
    "        \"criteria\": \"Does the summary focus on clinically relevant information?\",\n",
    "        \"score1_description\": \"The summary largely misses the clinically relevant details.\",\n",
    "        \"score2_description\": \"The summary includes only a few clinically relevant details.\",\n",
    "        \"score3_description\": \"The summary covers some relevant information but omits key aspects.\",\n",
    "        \"score4_description\": \"The summary covers most clinically relevant details with minor omissions.\",\n",
    "        \"score5_description\": \"The summary is entirely focused on clinically relevant information.\"\n",
    "    }\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenameid</th>\n",
       "      <th>clinical_case</th>\n",
       "      <th>discharge_summary</th>\n",
       "      <th>human_score</th>\n",
       "      <th>auto_score</th>\n",
       "      <th>prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31557359</td>\n",
       "      <td>A 61‐year‐old man, diabetic and hypertensive, ...</td>\n",
       "      <td>**Discharge Summary**\\n\\n**Patient Information...</td>\n",
       "      <td>{'Content Relevance': 5, 'Information Complete...</td>\n",
       "      <td>{'precision': 0.599999940000006, 'recall': 0.6...</td>\n",
       "      <td>Look at these guidelines carefully, i have als...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filenameid                                      clinical_case  \\\n",
       "7   31557359  A 61‐year‐old man, diabetic and hypertensive, ...   \n",
       "\n",
       "                                   discharge_summary  \\\n",
       "7  **Discharge Summary**\\n\\n**Patient Information...   \n",
       "\n",
       "                                         human_score  \\\n",
       "7  {'Content Relevance': 5, 'Information Complete...   \n",
       "\n",
       "                                          auto_score  \\\n",
       "7  {'precision': 0.599999940000006, 'recall': 0.6...   \n",
       "\n",
       "                                             prompts  \n",
       "7  Look at these guidelines carefully, i have als...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prompt.iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedback': 'The summary focuses on clinically relevant information, though minor details about sensing and pacing rhythms would be useful for optimizing therapy',\n",
       " 'score': 4}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prompt[\"generation\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_human_preds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\u001b[38;5;241m.\u001b[39massign(filenameid\u001b[38;5;241m=\u001b[39mdf_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenameid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 2\u001b[0m df_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prompt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39massign(filenameid\u001b[38;5;241m=\u001b[39mdf_prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenameid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      4\u001b[0m eval_metric \u001b[38;5;241m=\u001b[39m compute_correlations(df_human_preds, df_preds)\n\u001b[1;32m      5\u001b[0m log_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:837\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m--> 837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_dict_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m    839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:917\u001b[0m, in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    915\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[0;32m--> 917\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_unique_multiple_list_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(pre_cols)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# classes\u001b[39;00m\n",
      "File \u001b[0;32mlib.pyx:367\u001b[0m, in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03mConvert list of dicts to numpy arrays\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03mcolumns : Index\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    916\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data)\n\u001b[1;32m    917\u001b[0m     pre_cols \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_unique_multiple_list_gen(gen, sort\u001b[38;5;241m=\u001b[39msort)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "\n",
    "df_human_preds = pd.DataFrame(df_prompt[\"human_score\"].tolist()).assign(filenameid=df_prompt[\"filenameid\"].values)\n",
    "df_preds = pd.DataFrame(df_prompt[\"generation\"].tolist()).assign(filenameid=df_prompt[\"filenameid\"].values)\n",
    "\n",
    "eval_metric = compute_correlations(df_human_preds, df_preds)\n",
    "log_info(f\"Evaluation Metric: {eval_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, df_hm_llm_corr = plot_correlation_heatmap(df_human_preds, df_preds)\n",
    "\n",
    "log_info(\"\")\n",
    "log_info(f\"Saving results to {OUTPUT_PATH}\")\n",
    "    \n",
    "fig.savefig(os.path.join(OUTPUT_PATH, \"correlation_heatmap.png\"))\n",
    "df_human_preds.to_csv(os.path.join(OUTPUT_PATH, \"human_predictions.csv\"), index=False)\n",
    "df_preds.to_csv(os.path.join(OUTPUT_PATH, \"llm_predictions.csv\"), index=False)\n",
    "df_hm_llm_corr.to_csv(os.path.join(OUTPUT_PATH, \"correlation_matrix.csv\"), index=True)\n",
    "df_prompt.to_csv(os.path.join(OUTPUT_PATH, \"prompt_data.csv\"), index=False)\n",
    "\n",
    "\n",
    "if N_EXAMPLES > 0:\n",
    "    df_examples_human = pd.DataFrame(df_examples[\"human_score\"].tolist()).assign(filenameid=df_examples[\"filenameid\"].values)\n",
    "    df_examples_preds = df_prompt[df_prompt[\"filenameid\"].isin(df_examples[\"filenameid\"].values)]\n",
    "    df_examples_preds = pd.DataFrame(df_examples_preds[\"generation\"].tolist()).assign(filenameid=df_examples_preds[\"filenameid\"].values)\n",
    "    df_examples_auto = pd.DataFrame(df_examples[\"auto_score\"].tolist()).assign(filenameid=df_examples[\"filenameid\"].values)\n",
    "    \n",
    "    df_examples_human.to_csv(os.path.join(OUTPUT_PATH, \"examples_human_eval.csv\"), index=False)\n",
    "    df_examples_preds.to_csv(os.path.join(OUTPUT_PATH, \"examples_predictions.csv\"), index=False)\n",
    "    df_examples_auto.to_csv(os.path.join(OUTPUT_PATH, \"examples_auto_eval.csv\"), index=False)\n",
    "    \n",
    "log_info(f\"Results saved to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
